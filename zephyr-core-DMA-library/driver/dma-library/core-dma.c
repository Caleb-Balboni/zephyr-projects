#define DT_DRV_COMPAT hn_coredma

#include <zephyr/kernel.h>
#include <zephyr/sys/clock.h>
#include <zephyr/devicetree.h>
#include <zephyr/drivers/mbox.h>
#include <zephyr/sys/atomic.h>
#include <zephyr/device.h>
#include <stdbool.h>
#include <core-dma.h>

#define CHAN_TABLE_STATE_UNINIT 0
#define CHAN_TABLE_STATE_INPROGRESS 1
#define CHAN_TABLE_STATE_FINISHED 2

#define ALIGN_UP(x, y) (((x) + ((y) - 1)) & ~((y) - 1))

static int send_impl(const struct device* dev, void* data, size_t data_size);
static int async_receive_impl(const struct device* dev, void (*callback_func)(void*, void*, size_t), void* user_data);
static int sync_receive_impl(const struct device* dev, void* data, size_t data_size, k_timeout_t timeout);

void get_channel_table(const struct device* dev, struct dma_channel_table** table) {
  const struct dma_engine_cfg* cfg = (const struct dma_engine_cfg*)dev->config;
  *table = (struct dma_channel_table*)cfg->smem_base_adr;
  return;
}

static int dma_core_atomic_lock(atomic_t *lock, k_timeout_t timeout)
{
  if (K_TIMEOUT_EQ(timeout, K_NO_WAIT)) {
    return atomic_cas(lock, 0, 1) ? 0 : -EBUSY;
  }
  if (K_TIMEOUT_EQ(timeout, K_FOREVER)) {
    while (!atomic_cas(lock, 0, 1)) {
      k_yield();
    }
 return 0;
  }
  const int64_t start = k_uptime_ticks();
  const int64_t limit = timeout.ticks;
  if (limit <= 0) {
    return atomic_cas(lock, 0, 1) ? 0 : -EBUSY;
  }
  while (!atomic_cas(lock, 0, 1)) {
    if ((k_uptime_ticks() - start) >= limit) {
      return -ETIMEDOUT;
    }
    k_yield();
  }
  return 0;
}

static int dma_core_atomic_unlock(atomic_t* lock) {
	return atomic_set(lock, 0);
}

static void init_channel_table(struct dma_channel_table* c_table) {
  int table_state = atomic_get(&c_table->init_state);
  if (table_state == CHAN_TABLE_STATE_FINISHED) { return; }

  if (table_state != CHAN_TABLE_STATE_UNINIT && 
      table_state != CHAN_TABLE_STATE_INPROGRESS && 
      table_state != CHAN_TABLE_STATE_FINISHED) {
    atomic_set(&c_table->init_state, CHAN_TABLE_STATE_UNINIT);
  }
  if (atomic_cas(&c_table->init_state, CHAN_TABLE_STATE_UNINIT, CHAN_TABLE_STATE_INPROGRESS)) {
    atomic_set(&c_table->chan_lock, 0);
    for (uint8_t i = 0; i < CHAN_AMT; i++) {
      struct dma_channel_table_entry* entry = &c_table->channels[i];
      entry->available = 1;
      entry->chan_id = -1;
      entry->chan_tx_adr = NULL;
      entry->chan_rx_adr = NULL;
    }
    atomic_set(&c_table->init_state, CHAN_TABLE_STATE_FINISHED);
    return;
  }
  while (atomic_get(&c_table->init_state) != CHAN_TABLE_STATE_FINISHED) {
    k_yield();
  }
  return;
}

static int init_core_dma_engine(const struct device* dev, uint8_t chan_id) {
  const struct dma_engine_cfg* cfg = (const struct dma_engine_cfg*)dev->config;
  struct dma_engine_data* dma_data = (struct dma_engine_data*)dev->data;

  if (chan_id >= (cfg->chan_amt)) {
    return -1;
  }
  struct dma_channel_table* c_table = (struct dma_channel_table*)cfg->smem_base_adr;
  init_channel_table(c_table);
  dma_core_atomic_lock(&c_table->chan_lock, K_FOREVER);

  struct dma_channel_table_entry* chan_info = &c_table->channels[chan_id];
  if (chan_info->available) {
    uintptr_t chan_rx_adr = ALIGN_UP((uintptr_t)(cfg->smem_base_adr + sizeof(struct dma_channel_table) + (cfg->chan_size * chan_id)), 8);
    uintptr_t chan_tx_adr = ALIGN_UP((uintptr_t)(chan_rx_adr + (cfg->chan_size / 2)), 8);
    if ((cfg->smem_base_adr + cfg->smem_total_size) < (uint8_t*)chan_rx_adr + cfg->chan_size) {
      dma_core_atomic_unlock(&c_table->chan_lock);
      return -1;
    }
    chan_info->chan_rx_adr = (uint8_t*)chan_rx_adr; 
    chan_info->chan_tx_adr = (uint8_t*)chan_tx_adr;
    chan_info->chan_id = chan_id; 
    chan_info->available = 0;
    struct dma_channel_info* tx = (struct dma_channel_info*)chan_info->chan_tx_adr;
    struct dma_channel_info* rx = (struct dma_channel_info*)chan_info->chan_rx_adr;
    atomic_set(&tx->seq, 0);
    atomic_set(&tx->ack, 0);
    atomic_set(&rx->seq, 0);
    atomic_set(&rx->ack, 0);
  }
  if (cfg->is_master) {
    dma_data->rx = (struct dma_channel_info*)chan_info->chan_rx_adr;
    dma_data->tx = (struct dma_channel_info*)chan_info->chan_tx_adr;
  } else {
    dma_data->rx = (struct dma_channel_info*)chan_info->chan_tx_adr;
    dma_data->tx = (struct dma_channel_info*)chan_info->chan_rx_adr;
  }
  dma_core_atomic_unlock(&c_table->chan_lock);
  return 0;
}

static void async_receive_thread(void *p1, void *p2, void *p3) {
  const struct device *dev = (const struct device *)p1;
  void (*callback_func)(void*, void*, size_t) = (void (*)(void*, void*, size_t))p2;
  void *user_data = p3;
  struct dma_engine_data *dma_data = (struct dma_engine_data *)dev->data;
  const struct dma_engine_cfg *cfg = (const struct dma_engine_cfg *)dev->config;
  struct dma_channel_info *rx = dma_data->rx;

  for (;;) {
    if (atomic_get(&rx->seq) - 1 == atomic_get(&rx->ack)) {
      size_t copy_size = (cfg->chan_size / 2) - offsetof(struct dma_channel_info, data);
      void *data = (void *)rx->data;
      atomic_inc(&rx->ack);
      callback_func(data, user_data, copy_size);
      return;
    }
    k_yield();
  }
}

// recieves data from the other core and calls a user defined function containing the received data
// @param dev - the device aka the mbox and shared data region
// @param callback_func - the callback function passed by the user @arg1 - received data @arg2 - user data
// @param data_size - the amount of data to cpy from shared memory back to the user in bytes
// @param user_data - the given user data
// @return - 0 on success an error code on failure (zephyr standard) and those defined above
K_THREAD_STACK_DEFINE(async_stack, 1024);
static struct k_thread async_thread;

static int async_receive_impl(const struct device *dev, void (*callback_func)(void*, void*, size_t), void *user_data) {
    k_thread_create(&async_thread,
                    async_stack,
                    K_THREAD_STACK_SIZEOF(async_stack),
                    async_receive_thread,
                    (void *)dev,
                    (void *)callback_func,
                    user_data,
                    0, 0, K_NO_WAIT);
    return 0;
}

// recieves data and blocks the current thread until data has been received
// @param dev - the device using this api
// @param data - the input buffer from which data will be received by the user
// @param data_size - the amount of data the user wants to grab from the shared memory
// @param timeout - the amount of time for the function to block the thread before returning, uses
// standard zephyr time macros eg: K_FOREVER, K_MINUTES, K_MSEC ...
// @return - 0 if execution is successfull or a standard zephyr error code upon failure
static int sync_receive_impl(const struct device* dev, void* data, size_t data_size, k_timeout_t timeout) {
  struct dma_engine_data* dma_data = (struct dma_engine_data*)dev->data;
  struct dma_channel_info* rx = dma_data->rx; 
  if (K_TIMEOUT_EQ(timeout, K_NO_WAIT)) {
    if (rx->seq - 1 == rx->ack) {
      goto set_and_ret;
    }
  }
  if (K_TIMEOUT_EQ(timeout, K_FOREVER)) {
    for (;;) {
      if (atomic_get(&rx->seq) - 1 == atomic_get(&rx->ack)) {
        goto set_and_ret;
      }
    }
    return 0;
  }
  const int64_t start = k_uptime_ticks();
  const int64_t limit = timeout.ticks;
  if (limit <= 0) {
    if (atomic_get(&rx->seq) - 1 == atomic_get(&rx->ack)) {
      goto set_and_ret;
    }
  }
  for (;;) {
    if (atomic_get(&rx->seq) - 1 == atomic_get(&rx->ack)) {
      goto set_and_ret;
    }
    if ((k_uptime_ticks() - start) >= limit) {
      return -ETIMEDOUT;
    }
    k_yield();
  }
  set_and_ret:
  memcpy(data, rx->data, data_size); 
  atomic_inc(&rx->ack);
	return 0;
}

// sends data by notifying the other core and writing data to the shared memory pool
// @param dev - the device activly using this driver
// @param data - the user data that will be written to the shared memory
// @param data_size - the size of the data the user is passing (must be under pool size)
// @return - 0 upon success or a standard zephyr error code
static int send_impl(const struct device* dev, void* data, size_t data_size) {
	struct dma_engine_data* dma_data = (struct dma_engine_data*)dev->data;
  struct dma_channel_info* tx = dma_data->tx;
  if (atomic_get(&tx->seq) != atomic_get(&tx->ack)) {
    return -1;
  }
  memcpy(tx->data, data, data_size);
  atomic_inc(&tx->seq);
	return 0;
}

static const struct dma_engine dma_engine_api = {
	.sync_receive = sync_receive_impl,
	.async_receive = async_receive_impl,
	.send = send_impl,
	.init = init_core_dma_engine
};

#define COREDMA_DEFINE(inst)									                                                                \
	static struct dma_engine_data dma_engine_data_##inst;                                                       \
	static const struct dma_engine_cfg dma_engine_cfg_##inst = {				                                        \
		.is_master = DT_INST_PROP_OR(inst, is_master, 0),					                                                \
		.smem_base_adr = (uint8_t *)DT_REG_ADDR(DT_INST_PHANDLE(inst, memory_region)),	                          \
		.smem_total_size = (size_t)DT_REG_SIZE(DT_INST_PHANDLE(inst, memory_region)),		                          \
    .chan_size = (size_t)DT_INST_PROP(inst, chan_size),                                                       \
    .chan_amt = (uint8_t)DT_INST_PROP(inst, chan_amt),                                                        \
	};											                                                                                    \
	DEVICE_DT_INST_DEFINE(inst,								                                                                  \
			      NULL, NULL,							                                                                          \
			      &dma_engine_data_##inst,						                                                              \
			      &dma_engine_cfg_##inst,						                                                                \
			      POST_KERNEL, CONFIG_KERNEL_INIT_PRIORITY_DEVICE,			                                            \
			      &dma_engine_api);							                                                                    \

DT_INST_FOREACH_STATUS_OKAY(COREDMA_DEFINE)
